<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Mingxuan Li</title><meta name="author" content="Mingxuan Li"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Mingxuan Li</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Entrepreneurship"> Entrepreneurship</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Mingxuan Li</h3><p class="author-bio">Go forth, then, and do great work.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-links"><li><a class="e-social-link" href="https://scholar.google.com/citations?user=JZN_dKwAAAAJ&amp;hl" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://www.researchgate.net/profile/Mingxuan-Li-13" target="_blank"><i class="fab fa-researchgate" aria-hidden="true"></i><span>ResearchGate</span></a></li><li><a class="e-social-link" href="https://orcid.org/0000-0003-4159-1831" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li><li><a class="e-social-link" href="/img/wechat.jpg" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i><span>Wechat</span></a></li><li><a class="e-social-link" href="mailto:[mingxuan-li@foxmail.com]" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i><span>Email</span></a></li></ul></div><a class="cv-links" href="/attaches/CV_Mingxuan.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Publications</h2><article><h2 id="Publications"><a href="#Publications" class="headerlink" title="Publications:"></a>Publications:</h2><h4 id="7-EasyCalib-Simple-and-Low-Cost-In-Situ-Calibration-for-Force-Reconstruction-with-Vision-Based-Tactile-Sensors-Publication-Preprint-PDF-EasyCalib"><a href="#7-EasyCalib-Simple-and-Low-Cost-In-Situ-Calibration-for-Force-Reconstruction-with-Vision-Based-Tactile-Sensors-Publication-Preprint-PDF-EasyCalib" class="headerlink" title="[7] EasyCalib: Simple and Low-Cost In-Situ Calibration for Force Reconstruction with Vision-Based Tactile Sensors  [Publication] [Preprint_PDF] [EasyCalib]"></a>[7] <strong><font color=#ED7D31>EasyCalib: Simple and Low-Cost In-Situ Calibration for Force Reconstruction with Vision-Based Tactile Sensors</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1109/lra.2024.3426383"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_7.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>] [<a href="../attach/EasyCalib.pdf"><font color=#5B9BD5>EasyCalib</font></a>]</h4><center>
    <img src="../img/Fig7.webp" width="500px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Lunwei. Zhang, Yen. Hang. Zhou, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <strong><font color=#5B9BD5><em>IEEE Robotics and Automation Letters</em></font></strong>, Jul. 2024.</li>
<li>This article describes an in-situ calibration device, EasyCalib, for routinely measuring mechanical parameters (Young’s modulus and Poisson’s ratio) of visuotactile sensors. Detailed derivations of theories ensure that it is low-cost, user-friendly, and does not require F&#x2F;T sensors.</li>
</ul>
<h4 id="6-OneTip-A-soft-tactile-interface-for-6-D-fingertip-pose-acquisition-in-human-computer-interaction-Publication-Preprint-PDF-Video"><a href="#6-OneTip-A-soft-tactile-interface-for-6-D-fingertip-pose-acquisition-in-human-computer-interaction-Publication-Preprint-PDF-Video" class="headerlink" title="[6] OneTip: A soft tactile interface for 6-D fingertip pose acquisition in human-computer interaction  [Publication] [Preprint_PDF] [Video]"></a>[6] <strong><font color=#ED7D31>OneTip: A soft tactile interface for 6-D fingertip pose acquisition in human-computer interaction</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.sna.2024.115896"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_6.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>] [<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=yk25l4Vm9zs"><font color=#5B9BD5>Video</font></a>]</h4><center>
    <img src="../img/Fig6.webp" width="500px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Yen. Hang. Zhou, Lunwei. Zhang, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <font color=#5B9BD5><em><strong>Sensors and Actuators: A. Physical</strong></em></font>, Sep. 2024.</li>
<li>This article introduces visuotactile sensing technology into the field of human-computer interaction. We present a novel HCI device, OneTip, which can achieve 6-DOF input with just one fingertip. Evaluations and application explorations were conducted to explore the performance and usability of OneTip.</li>
</ul>
<h4 id="5-Incipient-Slip-Based-Rotation-Measurement-via-Visuotactile-Sensing-During-In-Hand-Object-Pivoting-Publication-Preprint-PDF-Video-Poster-Slides"><a href="#5-Incipient-Slip-Based-Rotation-Measurement-via-Visuotactile-Sensing-During-In-Hand-Object-Pivoting-Publication-Preprint-PDF-Video-Poster-Slides" class="headerlink" title="[5] Incipient Slip-Based Rotation Measurement via Visuotactile Sensing During In-Hand Object Pivoting  [Publication] [Preprint_PDF] [Video] [Poster] [Slides]"></a>[5] <strong><font color=#ED7D31>Incipient Slip-Based Rotation Measurement via Visuotactile Sensing During In-Hand Object Pivoting</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICRA57147.2024.10610988"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_5.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>] [<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1om421j7MH/?spm_id_from=333.999.0.0"><font color=#5B9BD5>Video</font></a>] [<a href="../attach/Poster_5.pdf"><font color=#5B9BD5>Poster</font></a>] [<a href="../attach/Slides_5.pdf"><font color=#5B9BD5>Slides</font></a>]</h4><center>
    <img src="../img/Fig5.webp" width="500px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Yen. Hang. Zhou, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <font color=#5B9BD5><em><strong>2024 IEEE International Conference on Robotics and Automation (ICRA)</strong></em></font>, Aug. 2024.</li>
<li>This paper describes a generalized 2-d contact model under pivoting, and proposes a measurement method of rotation angle based on the line features in the stick region. It could achieve the average measurement errors of 0.17°±0.15° (static) and 1.34°±0.48° (dynamic).</li>
</ul>
<h4 id="4-Real-time-and-Robust-Feature-Detection-of-Continuous-Marker-Pattern-for-Dense-3-D-Deformation-Measurement-Publication-Preprint-PDF"><a href="#4-Real-time-and-Robust-Feature-Detection-of-Continuous-Marker-Pattern-for-Dense-3-D-Deformation-Measurement-Publication-Preprint-PDF" class="headerlink" title="[4] Real-time and Robust Feature Detection of Continuous Marker Pattern for Dense 3-D Deformation Measurement  [Publication] [Preprint_PDF]"></a>[4] <strong><font color=#ED7D31>Real-time and Robust Feature Detection of Continuous Marker Pattern for Dense 3-D Deformation Measurement</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.measurement.2023.113479"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_4.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>]</h4><center>
    <img src="../img/Fig4.webp" width="500px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Yen. Hang. Zhou, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <strong><font color=#5B9BD5><em>Measurement</em></font></strong>, Nov. 2023.</li>
<li>This article achieves the measurement of dense 3-d contact deformation (10.7 markers per square millimeter). We propose a feature detection method applicable to visuotactile sensors based on continuous marker patterns (CMP), which reflects a clear superiority in real-time and reliability performance.</li>
</ul>
<h4 id="3-Improving-the-Representation-and-Extraction-of-Contact-Information-in-Vision-based-Tactile-Sensors-Using-Continuous-Marker-Pattern-Publication-Preprint-PDF"><a href="#3-Improving-the-Representation-and-Extraction-of-Contact-Information-in-Vision-based-Tactile-Sensors-Using-Continuous-Marker-Pattern-Publication-Preprint-PDF" class="headerlink" title="[3] Improving the Representation and Extraction of Contact Information in Vision-based Tactile Sensors Using Continuous Marker Pattern  [Publication] [Preprint_PDF]"></a>[3] <strong><font color=#ED7D31>Improving the Representation and Extraction of Contact Information in Vision-based Tactile Sensors Using Continuous Marker Pattern</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1109/lra.2023.3303830"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_3.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>]</h4><center>
    <img src="../img/Fig3.webp" width="400px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Yen. Hang. Zhou, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <strong><font color=#5B9BD5><em>IEEE Robotics and Automation Letters</em></font></strong>, Sep. 2023.</li>
<li>This article highlights the importance of raw representation and extraction in visuotactile perception. We propose a new multicolor CMP method (including pattern design, algorithm optimization, and preparation process) for enhancing the performance of vision-based tactile sensors.</li>
</ul>
<h4 id="2-Marker-Displacement-Method-Used-in-Vision-Based-Tactile-Sensors—From-2-D-to-3-D-A-Review-Publication-Preprint-PDF"><a href="#2-Marker-Displacement-Method-Used-in-Vision-Based-Tactile-Sensors—From-2-D-to-3-D-A-Review-Publication-Preprint-PDF" class="headerlink" title="[2] Marker Displacement Method Used in Vision-Based Tactile Sensors—From 2-D to 3-D: A Review  [Publication] [Preprint_PDF]"></a>[2] <strong><font color=#ED7D31>Marker Displacement Method Used in Vision-Based Tactile Sensors—From 2-D to 3-D: A Review</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1109/jsen.2023.3255861"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_2.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>]</h4><center>
    <img src="../img/Fig2.webp" width="400px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <strong><font color=#5B9BD5><em>IEEE Sensors Journal</em></font></strong>, Apr. 2023.</li>
<li>This article presents a detailed review and categorizing of the marker displacement method (MDM) used in vision-based tactile sensors. We classify MDM into three typical categories based on the dimensionality perspective for the first time: 2D MDM, 2.5D MDM, and 3D MDM.</li>
</ul>
<h4 id="1-Continuous-Marker-Patterns-for-Representing-Contact-Information-in-Vision-Based-Tactile-Sensor-Principle-Algorithm-and-Verification-Publication-Preprint-PDF"><a href="#1-Continuous-Marker-Patterns-for-Representing-Contact-Information-in-Vision-Based-Tactile-Sensor-Principle-Algorithm-and-Verification-Publication-Preprint-PDF" class="headerlink" title="[1] Continuous Marker Patterns for Representing Contact Information in Vision-Based Tactile Sensor: Principle, Algorithm, and Verification  [Publication] [Preprint_PDF]"></a>[1] <strong><font color=#ED7D31>Continuous Marker Patterns for Representing Contact Information in Vision-Based Tactile Sensor: Principle, Algorithm, and Verification</font></strong>  [<a target="_blank" rel="noopener" href="https://doi.org/10.1109/tim.2022.3196730"><font color=#5B9BD5>Publication</font></a>] [<a href="../attach/Preprint_1.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>]</h4><center>
    <img src="../img/Fig1.webp" width="400px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Lunwei. Zhang, Tiemin. Li, and Yao. Jiang</li>
<li>Published in <strong><font color=#5B9BD5><em>IEEE Transactions on Instrumentation and Measurement</em></font></strong>, Aug. 2022.</li>
<li>This article innovatively proposes the idea of the continuous marker pattern (CMP) and three basic design principles to optimize tactile representation and extraction in visuotactile sensors. Simulation and prototype experiments prove the tactile sensors with CMP outperformed the distributed marker pattern (DMP) regarding measurement precision, resolution, and reliability.</li>
</ul>
<h2 id="Pre-print"><a href="#Pre-print" class="headerlink" title="Pre-print:"></a>Pre-print:</h2><h4 id="1-Learning-Gentle-Grasping-from-Human-Free-Force-Control-Demonstration-arXiv-Preprint-PDF"><a href="#1-Learning-Gentle-Grasping-from-Human-Free-Force-Control-Demonstration-arXiv-Preprint-PDF" class="headerlink" title="[1] Learning Gentle Grasping from Human-Free Force Control Demonstration [arXiv] [Preprint_PDF]"></a>[1] <strong><font color=#ED7D31>Learning Gentle Grasping from Human-Free Force Control Demonstration</font></strong> [<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10371"><font color=#5B9BD5>arXiv</font></a>] [<a href="../attach/Preprint_8.pdf"><font color=#5B9BD5>Preprint_PDF</font></a>]</h4><center>
    <img src="../img/Fig8.webp" width="650px" />
</center>

<ul>
<li><strong>Mingxuan. Li</strong>, Lunwei. Zhang, Tiemin. Li, and Yao. Jiang</li>
<li>Submitted to <strong><font color=#5B9BD5><em>IEEE Robotics and Automation Letters</em></font></strong>, Sep. 2024.</li>
<li>This article utilizes objects with known contact characteristics to automatically generate reference force curves without human demonstrations. The described method can be effectively applied in vision-based tactile sensors and teaches robots to gently and stably grasp objects from the ground.</li>
</ul>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Entrepreneurship"> Entrepreneurship</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2024 by Mingxuan Li</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>